{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for reproduced Laplace\n",
    "1. shape test\n",
    "2. examine if the results given by reproduced Laplace and `laplace-torch` package match.\n",
    "3. test on reproduced Laplace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from LABDL.laplace.curvatures import BackPackGGN as BPG\n",
    "from LABDL.laplace.curvatures import BackPackEF as BPE\n",
    "\n",
    "from laplace.curvature import BackPackEF, BackPackGGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 36, 20])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "A = torch.rand(10, 4, 4)\n",
    "B = torch.rand(7, 9, 5)\n",
    "t = torch.kron(A, B)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "in_features = 3\n",
    "out_features = 2\n",
    "\n",
    "def _model():\n",
    "    \"\"\"model to fit on\"\"\"\n",
    "    model = nn.Linear(in_features, out_features)\n",
    "    setattr(model, 'output_size', 2) # for the laplace-torch\n",
    "    setattr(model, 'num_params', sum([len(param.flatten()) for param in model.parameters()])\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "def _reg_Xy():\n",
    "    \"\"\"regression samples\"\"\"\n",
    "    X = torch.randn(batch_size, in_features)\n",
    "    y = torch.randn(batch_size, out_features)\n",
    "    return X, y\n",
    "\n",
    "def _cls_Xy():\n",
    "    \"\"\"classification samples\"\"\"\n",
    "    X = torch.randn(batch_size, in_features)\n",
    "    y = torch.randint(out_features, (batch_size, ))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _model()\n",
    "reg_Xy = _reg_Xy()\n",
    "cls_Xy = _cls_Xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients.shape: torch.Size([10, 8])\n",
      "jacobians.shape: torch.Size([10, 2, 8])\n",
      "ggn_full.shape: torch.Size([8, 8])\n",
      "ggn_diag.shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "def test_ggn_shape(model, Xy):\n",
    "    \"\"\"GGNCurvature methods shape test\n",
    "    Args: \n",
    "    model: note that in BackPack, self-defined module should be extended. \n",
    "    \"\"\"\n",
    "    X, y = Xy\n",
    "    ggn = BPG(model, likelihood='regression')\n",
    "\n",
    "    Gs, loss = ggn.gradients(X, y)\n",
    "    print(\"gradients.shape:\", Gs.shape)\n",
    "    assert Gs.shape == (batch_size, model.num_params)\n",
    "        \n",
    "    Js, out = ggn.jacobians(X)\n",
    "    print(\"jacobians.shape:\", Js.shape)\n",
    "    assert Js.shape == (batch_size, out_features, model.num_params)\n",
    "    \n",
    "    ggn_full, loss = ggn.full(X, y)\n",
    "    print(\"ggn_full.shape:\", ggn_full.shape)\n",
    "    assert ggn_full.shape == (model.num_params, model.num_params)\n",
    "                     \n",
    "    ggn_diag, loss = ggn.diag(X, y)\n",
    "    print(\"ggn_diag.shape:\", ggn_diag.shape)\n",
    "    assert ggn_diag.shape == (model.num_params,)\n",
    "        \n",
    "    #print(\"=====kron=====\")\n",
    "    #gnn_kron, loss = ggn.kron(features, targets)\n",
    "\n",
    "test_ggn_shape(model, reg_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ef_full.shape: torch.Size([8, 8])\n",
      "ef_diag.shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "def test_ef_shape(model, Xy):\n",
    "    \"\"\"EFCurvature methods shape test\n",
    "    Args: \n",
    "    model: note that in BackPack, self-defined module should be extended. \n",
    "    \"\"\"\n",
    "    X, y = Xy\n",
    "    ef = BPE(model, likelihood='regression')\n",
    "\n",
    "    ef_full, loss = ef.full(X, y)\n",
    "    print(\"ef_full.shape:\", ef_full.shape)\n",
    "    assert ef_full.shape == (model.num_params, model.num_params)\n",
    "                     \n",
    "    ef_diag, loss = ef.diag(X, y)\n",
    "    print(\"ef_diag.shape:\", ef_diag.shape)\n",
    "    assert ef_diag.shape == (model.num_params,)\n",
    "        \n",
    "    #print(\"=====kron=====\")\n",
    "    #gnn_kron, loss = ef.kron(features, targets)\n",
    "\n",
    "test_ef_shape(model, reg_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ef(model, Xy, likelihood): \n",
    "    X, y = Xy\n",
    "    ef = BPE(model, likelihood)\n",
    "    efbp = BackPackEF(model, likelihood) \n",
    "    H_ef, loss = ef.full(X, y)     \n",
    "    bploss, H_efbp = efbp.full(X, y)\n",
    "    np.testing.assert_allclose(loss, bploss, atol=1e-8) \n",
    "    np.testing.assert_allclose(H_ef, H_efbp, atol=1e-8) \n",
    "    \n",
    "    H_ef, loss = ef.diag(X, y)     \n",
    "    bploss, H_efbp = efbp.diag(X, y)\n",
    "    np.testing.assert_allclose(loss, bploss, atol=1e-8) \n",
    "    np.testing.assert_allclose(H_ef, H_efbp, atol=1e-8) \n",
    "    \n",
    "    \n",
    "test_ef(model, reg_Xy, 'regression')\n",
    "test_ef(model, reg_Xy, 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ggn(model, Xy, likelihood, backend_kwargs): \n",
    "    X, y = Xy\n",
    "    ggn = BPG(model, likelihood, **backend_kwargs)\n",
    "    ggnbp = BackPackGGN(model, likelihood, **backend_kwargs) \n",
    "    \n",
    "    #H, loss = ggn.full(X, y)     \n",
    "    #bploss, bpH = ggnbp.full(X, y)\n",
    "    #H, bpH = H.detach().numpy(), bpH.detach().numpy()\n",
    "        \n",
    "    #np.testing.assert_allclose(loss, bploss, atol=1e-6) \n",
    "    #np.testing.assert_allclose(H, bpH, atol=1e-6) \n",
    "    \n",
    "    H, loss = ggn.diag(X, y)     \n",
    "    bploss, bpH = ggnbp.diag(X, y)\n",
    "    H, bpH = H.detach().numpy(), bpH.detach().numpy()\n",
    "    \n",
    "    np.testing.assert_allclose(loss, bploss, atol=1e-6) \n",
    "    np.testing.assert_allclose(H, bpH, atol=1e-6) \n",
    "    \n",
    "backend_kwargs = {'stochastic':False}\n",
    "test_ggn(model, reg_Xy, 'regression', backend_kwargs)\n",
    "test_ggn(model, cls_Xy, 'classification', backend_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EF, and GGN with MC approximation might show non-neglectable deviations from the exact GGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 8 / 8 (100%)\nMax absolute difference: 1.1201124\nMax relative difference: 0.3260285\n x: array([9.239404, 2.315515, 3.606776, 9.239404, 2.315515, 3.606776,\n       9.999999, 9.999999], dtype=float32)\n y: array([ 8.538967,  2.393187,  4.290271,  9.51006 ,  3.435627,  3.244742,\n       10.883088, 10.20693 ], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(eloss, sloss, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[1;32m     15\u001b[0m     np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(eH, sH, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[0;32m---> 17\u001b[0m \u001b[43mtest_ggn_exact_vs_stochastic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_Xy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mtest_ggn_exact_vs_stochastic\u001b[0;34m(model, Xy)\u001b[0m\n\u001b[1;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(bpH, eH, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[1;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(eloss, sloss, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[0;32m---> 15\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43meH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/share/apps/anaconda-ci/fgci-centos7-anaconda/software/anaconda/2023-03/6a700484/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/apps/anaconda-ci/fgci-centos7-anaconda/software/anaconda/2023-03/6a700484/lib/python3.10/site-packages/numpy/testing/_private/utils.py:797\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    793\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    794\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    795\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    796\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 8 / 8 (100%)\nMax absolute difference: 1.1201124\nMax relative difference: 0.3260285\n x: array([9.239404, 2.315515, 3.606776, 9.239404, 2.315515, 3.606776,\n       9.999999, 9.999999], dtype=float32)\n y: array([ 8.538967,  2.393187,  4.290271,  9.51006 ,  3.435627,  3.244742,\n       10.883088, 10.20693 ], dtype=float32)"
     ]
    }
   ],
   "source": [
    "def test_ggn_exact_vs_stochastic(model, Xy): \n",
    "    X, y = Xy\n",
    "    ggn_exact = BPG(model, 'regression', stochastic=False)\n",
    "    ggn_stoch = BPG(model, 'regression', stochastic=True)\n",
    "    ggnbp = BackPackGGN(model, 'regression', stochastic=False)\n",
    "    \n",
    "    eH, eloss = ggn_exact.diag(X, y)\n",
    "    sH, sloss = ggn_stoch.diag(X, y)\n",
    "    bploss, bpH = ggnbp.diag(X, y)\n",
    "    \n",
    "    np.testing.assert_allclose(bploss, eloss, atol=1e-8) \n",
    "    np.testing.assert_allclose(bpH, eH, atol=1e-8) \n",
    "    \n",
    "    np.testing.assert_allclose(eloss, sloss, atol=1e-8) \n",
    "    np.testing.assert_allclose(eH, sH, atol=1e-8) \n",
    "    \n",
    "test_ggn_exact_vs_stochastic(model, reg_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 64 / 64 (100%)\nMax absolute difference: 2.6726496\nMax relative difference: 4.7019544\n x: array([[ 4.443159,  2.600602,  0.798379, -4.443159, -2.600602, -0.798379,\n         1.348529, -1.348529],\n       [ 2.600602,  4.864106,  1.219426, -2.600602, -4.864106, -1.219426,...\n y: array([[ 3.418147,  0.920167,  0.629899, -3.418147, -0.920167, -0.629899,\n         0.389323, -0.389323],\n       [ 0.920167,  2.191456,  0.424093, -0.920167, -2.191456, -0.424093,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(efloss, ggnloss, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[1;32m     11\u001b[0m     np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(H_ef, H_ggn, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[0;32m---> 14\u001b[0m \u001b[43mtest_ef_full_vs_ggn_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_Xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m test_ef_full_vs_ggn_full(model, reg_Xy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtest_ef_full_vs_ggn_full\u001b[0;34m(model, Xy, likelihood)\u001b[0m\n\u001b[1;32m      8\u001b[0m H_ef, H_ggn \u001b[38;5;241m=\u001b[39m H_ef\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), H_ggn\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(efloss, ggnloss, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \n\u001b[0;32m---> 11\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_ef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_ggn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/share/apps/anaconda-ci/fgci-centos7-anaconda/software/anaconda/2023-03/6a700484/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/apps/anaconda-ci/fgci-centos7-anaconda/software/anaconda/2023-03/6a700484/lib/python3.10/site-packages/numpy/testing/_private/utils.py:797\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    793\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    794\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    795\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    796\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 64 / 64 (100%)\nMax absolute difference: 2.6726496\nMax relative difference: 4.7019544\n x: array([[ 4.443159,  2.600602,  0.798379, -4.443159, -2.600602, -0.798379,\n         1.348529, -1.348529],\n       [ 2.600602,  4.864106,  1.219426, -2.600602, -4.864106, -1.219426,...\n y: array([[ 3.418147,  0.920167,  0.629899, -3.418147, -0.920167, -0.629899,\n         0.389323, -0.389323],\n       [ 0.920167,  2.191456,  0.424093, -0.920167, -2.191456, -0.424093,..."
     ]
    }
   ],
   "source": [
    "def test_ef_full_vs_ggn_full(model, Xy, likelihood): \n",
    "    X, y = Xy\n",
    "    ef = BPE(model, likelihood)\n",
    "    ggn = BPG(model, likelihood)\n",
    "    \n",
    "    H_ef, efloss = ef.full(X, y)\n",
    "    H_ggn, ggnloss = ggn.full(X, y)\n",
    "    H_ef, H_ggn = H_ef.detach().numpy(), H_ggn.detach().numpy()\n",
    "    \n",
    "    np.testing.assert_allclose(efloss, ggnloss, atol=1e-8) \n",
    "    np.testing.assert_allclose(H_ef, H_ggn, atol=1e-8) \n",
    "    \n",
    "    \n",
    "test_ef_full_vs_ggn_full(model, cls_Xy, 'classification')\n",
    "test_ef_full_vs_ggn_full(model, reg_Xy, 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_vs_diag_ef(model, Xy, likelihood):\n",
    "    X, y = Xy\n",
    "    ef = BPE(model, likelihood)\n",
    "    \n",
    "    dH, dloss = ef.diag(X, y)\n",
    "    fH, floss = ef.full(X, y)\n",
    "    \n",
    "    assert dloss == floss\n",
    "    assert torch.allclose(dH, fH.diagonal())\n",
    "    \n",
    "test_full_vs_diag_ef(model, reg_Xy, 'regression')\n",
    "test_full_vs_diag_ef(model, cls_Xy, 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_vs_diag_ggn(model, Xy, likelihood):\n",
    "    X, y = Xy\n",
    "    ggn = BPG(model, likelihood)\n",
    "    \n",
    "    fH, floss = ggn.full(X, y)  \n",
    "    dH, dloss = ggn.diag(X, y)\n",
    "    fH, dH = fH.detach().numpy(), dH.detach().numpy()\n",
    "\n",
    "    np.testing.assert_allclose(fH.diagonal(), dH, atol=1e-6) \n",
    "    np.testing.assert_allclose(floss, dloss) \n",
    "\n",
    "test_full_vs_diag_ggn(model, reg_Xy, 'regression')\n",
    "test_full_vs_diag_ggn(model, cls_Xy, 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size:  10\n",
      "======jacobians using retain_graph=====\n",
      "time:  0.003782510757446289\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def test_jacobians_time(model, Xy):\n",
    "    \"\"\"\n",
    "    time test when computing jacobians by setting `retain_graph` as True or forwarding for every output dimension. \n",
    "    \"\"\"\n",
    "    X, y = Xy\n",
    "    ggn = BPG(model, 'regression', stochastic=False)\n",
    "    print(\"test set size: \", len(X))\n",
    "    print(\"======jacobians using retain_graph=====\")\n",
    "    start = time.time()\n",
    "    Js, out = ggn.jacobians(X)\n",
    "    end = time.time()\n",
    "    time_Js = end-start\n",
    "    print(\"time: \", time_Js)\n",
    "    \n",
    "    #print(\"======jacobians not using retain_graph=====\")\n",
    "    #start = time.time()\n",
    "    #Js_wr, out_wr = ggn.jacobians_without_retain(X)\n",
    "    #end = time.time()\n",
    "    #time_Jswr = end-start\n",
    "    #print(\"time: \", time_Jswr)\n",
    "    \n",
    "    #print(\"ratio: \", time_Js / time_Jswr)\n",
    "    \n",
    "    #np.testing.assert_allclose(Js.detach().numpy(), Js_wr.detach().numpy())\n",
    "    #np.testing.assert_allclose(out.detach().numpy(), out_wr.detach().numpy())\n",
    "\n",
    "test_jacobians_time(model, reg_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
